# Erick's PhD Research
This repository maintains a list of papers and materials Erick uses in his research

## [Content](#content)

<table>
<tr><td colspan="2"><a href="#Papers">1. Papers </a></td></tr>
 <tr><td colspan="2">&emsp;<a href="#Change-Point-Detection">1.1 Change Point Detection</a></td></tr>
<tr><td colspan="2">&emsp;<a href="#Anomaly-Detection">1.2 Anomaly Detection</a></td></tr>
<tr><td colspan="2">&emsp;<a href="#Data-Imputation">1.3 Data Imputation</a></td></tr>
<tr><td colspan="2">&emsp;<a href="#Un/Self-supervised-Learning">1.4 Un/Self-supervised Learning</a></td></tr>
<tr><td colspan="2">&emsp;<a href="#Knowledge-Distillation">1.5 Knowledge Distillation</a></td></tr>
<tr><td colspan="2">&emsp;<a href="#Neural-Architecture-Search">1.6 Neural Architecture Search</a></td></tr>
<tr><td colspan="2">&emsp;<a href="#Edge-Computing">1.7 Edge Computing</a></td></tr>
<tr><td colspan="2">&emsp;<a href="#Transformer">1.8 Transformer</a></td></tr>
<tr><td colspan="2">&emsp;<a href="#Attention">1.9 Attention</a></td></tr>

<tr><td colspan="2"><a href="#Courses">2. Courses </a></td></tr>
<tr><td colspan="2"><a href="#Notebooks">3. Notebooks </a></td></tr>
<tr><td colspan="2"><a href="#Github">4. Github </a></td></tr>
<tr><td colspan="2"><a href="#Books">5. Books </a></td></tr>
<tr><td colspan="2"><a href="#Foundations">6. Foundations </a></td></tr>
<tr><td colspan="2">&emsp;<a href="#Basics">6.1 Basics</a></td></tr>
<tr><td colspan="2">&emsp;<a href="#CNNs">6.2 CNNs</a></td></tr>
<tr><td colspan="2">&emsp;<a href="#RNNs">6.3 RNNs</a></td></tr>
<tr><td colspan="2">&emsp;<a href="#Embeddings">6.4 Embeddings</a></td></tr>
<tr><td colspan="2">&emsp;<a href="#Transformers">6.5 Transformers</a></td></tr>
<tr><td colspan="2">&emsp;<a href="#Attentions">6.6 Attentions</a></td></tr>

</table>

## [Papers](#content)
#### [Change Point Detection](#content)

1. **An Evaluation of Change Point Detection Algorithms.** arXiv.stat.ML 2020. [paper](https://arxiv.org/abs/2003.06222)
    *Gerrit J.J. van den Burg, Christopher K.I. Williams.*

1. **A Survey of Methods for Time Series Change Point Detection.** ACM 2017. [paper](https://www.researchgate.net/publication/307947624_A_Survey_of_Methods_for_Time_Series_Change_Point_Detection)
    *Samaneh Aminikhanghahi, Diane J Cook.*

1. **On the Performances of Trend and Change-Point Detection Methods for Remote Sensing Data.** MDPI 2020. [paper](https://www.mdpi.com/2072-4292/12/6/1008)
    *Ana F. Militino, Mehdi Moradi, M. Dolores Ugarte.*

1. **Bayesian Online Changepoint Detection.** arXiv.stat.ML 2007. [paper](https://arxiv.org/abs/0710.3742)
    *Ryan Prescott Adams, David J.C. MacKay.*

1. **Time Series Change Point Detection with Self-Supervised Contrastive Predictive Coding.** WWW'2021. [paper](https://arxiv.org/abs/2011.14097)
    *Shohreh Deldari, Daniel V. Smith, Hao Xue, Flora D. Salim.*
    

#### [Anomaly Detection](#content)

1. **Real-Time Anomaly Detection for Streaming Analytics.** arXiv 2016. [paper](https://arxiv.org/pdf/1607.02480.pdf)
    *Subutai Ahmad, Scott Purdy.*

1. **USAD: UnSupervised Anomaly Detection on Multivariate Time Series.** KDD 2020. [paper](https://dl.acm.org/doi/10.1145/3394486.3403392)
    *Julien Audibert , Pietro Michiardi , Frédéric Guyard , Sébastien Marti , Maria A. Zuluaga.*

1. **Detecting Suspicious Pattern Absences in Continuous Time Series.** SIAM 2020. [paper](https://epubs.siam.org/doi/10.1137/1.9781611976236.15)
    *JVincent Vercruyssen, Wannes Meert and Jesse Davis.*
    
1. **Trajectory Outlier Detection: Algorithms, Taxonomies,
Evaluation, and Open Challenges.** ACM 2020. [paper](https://kristiania.brage.unit.no/kristiania-xmlui/bitstream/handle/11250/2754894/Belhadi.pdf?sequence=1)
    *Asma Belhadi , Youcef Djenouri , Jerry Chun-Wei Lin , Alberto Cano.*

1. **An overview on trajectory outlier detection.** Springer 2018. [paper](https://www.researchgate.net/publication/322898173_An_overview_on_trajectory_outlier_detection)
    *Fanrong Meng.*

1. **Trajectory Outlier Detection: A Partition-and-Detect Framework.** IEEE 2008. [paper](http://hanj.cs.illinois.edu/pdf/icde08_jaegil_lee.pdf)
    *Jae-Gil Lee, Jiawei Han, Xiaolei Li.*


#### [Data Imputation](#content)

1. **Missing Data Imputation using Optimal Transport.** PMLR 2020. [paper](http://proceedings.mlr.press/v119/muzellec20a.html)
    *Boris Muzellec, Julie Josse, Claire Boyer, Marco Cuturi.*

1. **Filling Missing Values on Wearable-Sensory Time Series Data .** SIAM 2020. [paper](https://epubs.siam.org/doi/10.1137/1.9781611976236.6)
    *Suwen Lin, Xian Wu, Gonzalo Martinez and Nitesh V. Chawla.*
    
1. **ST-MVL: Filling Missing Values in Geo-Sensory Time Series Data.** IJCAI 2016. [paper](https://www.ijcai.org/Proceedings/16/Papers/384.pdf)
    *Xiuwen Yi, Yu Zheng, Junbo Zhang, Tianrui Li.*


#### [Un/Self-supervised Learning](#content)

1. **Transfer Learning or Self-supervised Learning? A Tale of Two Pretraining Paradigms.** arXiv 2020. [paper](https://arxiv.org/abs/2007.04234)
    *Xingyi Yang, Xuehai He, Yuxiao Liang, Yue Yang, Shanghang Zhang, Pengtao Xie.*


#### [Knowledge Distillation](#content)

1. **Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks.** IEEE 2021. [paper](https://arxiv.org/abs/2004.05937)
    *Lin Wang, Kuk-Jin Yoon.*

1. **Knowledge Distillation: A Survey.** arVix 2020. [paper](https://arxiv.org/abs/2004.05937)
    *Jianping Gou, Baosheng Yu, Stephen John Maybank, Dacheng Tao.*

1. **Online Knowledge Distillation with Diverse Peers.** AAAI 2020. [paper](https://www.semanticscholar.org/paper/Online-Knowledge-Distillation-with-Diverse-Peers-Chen-Mei/35d39c2f61277a89d09dc899fa467ade6a3789af)
    *Defang Chen, Jian-Ping Mei, Can Wang, Yan Feng, Chun Chen.*

1. **TinyBERT: Distilling BERT for Natural Language Understanding.** arVix 2019. [paper](https://arxiv.org/abs/1909.10351)
    *Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, Qun Liu.*

1. **FastBERT: a Self-distilling BERT with Adaptive Inference Time.** ACL 2020. [paper](https://arxiv.org/abs/2004.02178)
    *Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Haotang Deng, Qi Ju.*

1. **Improved Knowledge Distillation via Teacher Assistant.** AAAI 2020. [paper](https://www.semanticscholar.org/paper/Improved-Knowledge-Distillation-via-Teacher-Mirzadeh-Farajtabar/c4c703d1bc2ac6bdca7e76c8e2cbde0314579b9b)
    *S. Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, H. Ghasemzadeh.*

1. **Learning Lightweight Lane Detection CNNs by Self Attention Distillation.** arVix 2019. [paper](https://arxiv.org/abs/1908.00821)
    *Yuenan Hou, Zheng Ma, Chunxiao Liu, Chen Change Loy.*

1. **Distilling BERT into Simple Neural Networks with Unlabeled Transfer Data.** arVix 2019. [paper](https://arxiv.org/abs/1910.01769)
    *Subhabrata Mukherjee, Ahmed Hassan Awadallah.*

1. **Distilling Task-Specific Knowledge from BERT into Simple Neural Networks.** arVix 2019. [paper](https://arxiv.org/abs/1903.12136)
    *Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, Jimmy Lin.*

1. **Knowledge Distillation by On-the-Fly Native Ensemble.** NeurIPS 2018. [paper](https://www.semanticscholar.org/paper/Knowledge-Distillation-by-On-the-Fly-Native-Lan-Zhu/c864e3785a9aecf25296781c272980eaed78e51a)
    *Xu Lan, Xiatian Zhu, S. Gong.*

1. **Born-Again Neural Networks.** PMLR 2018. [paper](https://proceedings.mlr.press/v80/furlanello18a/furlanello18a.pdf)
    *Tommaso Furlanello, Zachary C. Lipton, Michael Tschannen, Laurent Itti, Anima Anandkumar.*

1. **Distilling a Neural Network Into a Soft Decision Tree.** arVix 2017. [paper](https://arxiv.org/abs/1711.09784)
    *Nicholas Frosst, Geoffrey Hinton.*

1. **Sequence-Level Knowledge Distillation.** IEEE 2016. [paper](https://arxiv.org/abs/1606.07947)
    *Yoon Kim, Alexander M. Rush.*

#### [Neural Architecture Search](#content)

1. **Neural Architecture Search: A Survey.** JMLR 2019. [paper](https://www.jmlr.org/papers/volume20/18-598/18-598.pdf)
    *Thomas Elsken, Jan Hendrik Metzen, Frank Hutter.*

1. **A Comprehensive Survey of Neural Architecture Search: Challenges and Solutions.** ACM 2020. [paper](https://arxiv.org/abs/2006.02903)
    *Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Xiaojiang Chen, Xin Wang.*

1. **BayesNAS: A Bayesian Approach for Neural Architecture Search.** arVix 2019. [paper](https://www.semanticscholar.org/paper/BayesNAS%3A-A-Bayesian-Approach-for-Neural-Search-Zhou-Yang/93c4478a5f6e22925ac0582105e4fb5a94e787a7)
    *Hongpeng Zhou, M. Yang, J. Wang, Wei Pan.*

1. **FP-NAS: Fast Probabilistic Neural Architecture Search.** CVPR 2021. [paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Yan_FP-NAS_Fast_Probabilistic_Neural_Architecture_Search_CVPR_2021_paper.pdf)
    *Zhicheng Yan, Xiaoliang Dai, Peizhao Zhang, Yuandong Tian, Bichen Wu, Matt Feiszli.*

1. **CP-NAS: Child-Parent Neural Architecture Search for 1-bit CNNs.** IJCAI 2020. [paper](https://arxiv.org/abs/2005.00057)
    *Li'an Zhuo, Baochang Zhang, Hanlin Chen, Linlin Yang, Chen Chen, Yanjun Zhu, David Doermann.*


#### [Edge Computing](#content)

1. **Edge Machine Learning for AI-Enabled IoT Devices: A Review.** MDPI 2020. [paper](https://www.mdpi.com/1424-8220/20/9/2533)
    *Massimo Merenda,Carlo Porcaro,  Demetrio Iero.*

1. **Deep Learning on Mobile Devices - A Review.** arVix 2019. [paper](https://arxiv.org/abs/1904.09274)
    *Yunbin Deng.*


#### [Transformer](#content)

1. **Efficient Transformers: A Survey.** arVix 2020. [paper](https://arxiv.org/abs/2009.06732)
    *Yi Tay, Mostafa Dehghani, Dara Bahri, Donald Metzler.*

1. **Long Range Arena: A Benchmark for Efficient Transformers.** arVix 2020. [paper](https://arxiv.org/abs/2011.04006)
    *Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, Donald Metzler.*

1. **Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting.** arVix 2020. [paper](https://arxiv.org/abs/2012.07436)
    *Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, Wancai Zhang.*

1. **Deep Transformer Models for Time Series Forecasting: The Influenza Prevalence Case.** arVix 2020. [paper](https://arxiv.org/abs/2001.08317)
    *Neo Wu, Bradley Green, Xue Ben, Shawn O'Banion.*

1. **The Evolved Transformer.** ICML 2019. [paper](https://arxiv.org/abs/1901.11117)
    *David R. So, Chen Liang, Quoc V. Le.*
    

#### [Attention](#content)

1. **Attention Is All You Need.** NeurlPS 2017. [paper](https://arxiv.org/abs/1706.03762)
    *Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin.*

1. **Effective Approaches to Attention-based Neural Machine Translation.** ACL 2015. [paper](https://aclanthology.org/D15-1166/)
    *Thang Luong, Hieu Pham, Christopher D. Manning.*


### [Notebook](#content)

1. **notebook_Trajectory Outlier Detection.**  [link](https://docs.google.com/document/d/1DlQEsF-5l1OMhmflhH29dJ3wL3DEzEWVExxA56admVI/edit?usp=sharing) For USDA Animal Science Project


### [Github](#content)

1. **Anomaly Detection Learning Resources.**  [link](https://github.com/yzhao062/anomaly-detection-resources)

1. **awesome-TS-anomaly-detection .**  [link](https://github.com/rob-med/awesome-TS-anomaly-detection)


### [Books](#content)

1. **Mathematical Foundations for Data Analysis.**  [link](https://mathfordata.github.io/)
    *Jeff M. Phillips*


### [Foundations](#content)


#### [Deep Learning](#content)

**🕐🕹 YouTube - Complete Deep Learning.**  [link](https://www.youtube.com/watch?v=mH9GBJ6og5A&list=PLZoTAELRMXVPGU70ZGsckrMdr0FteeRUi&index=7)  

1. **Basics.** [note](https://docs.google.com/document/d/1N_pN_1e2B6Ytjx7Xl_f6P0G8HnbyKovUPRdZ3V4U4wo/edit?usp=sharing) [code](https://colab.research.google.com/drive/1SVbHVL7SuUonq66Y9d7SW60r5O6qvjCI?usp=sharing)
